#!/usr/bin/env python3
"""
æµ‹è¯•1: æ‰¹é‡æ•°æ®åŠ è½½æ¢å¤æµ‹è¯• (æ–°æµç¨‹)
- 10ä¸‡æ¡æ•°æ®ï¼ŒTPS=100/s
- åŠ è½½è¿‡ç¨‹ä¸­æ‰§è¡Œå…¨é‡å¤‡ä»½
- å®Œæˆååˆ é™¤ç›®æ ‡è¡¨
- ä»æœ€æ–°exportæ¢å¤
- ä½¿ç”¨enhanced_batch_applieråº”ç”¨å¢é‡å˜æ›´
- éªŒè¯æ•°æ®ä¸€è‡´æ€§
"""

import boto3
import json
import time
import threading
import logging
from datetime import datetime, timezone
from concurrent.futures import ThreadPoolExecutor, as_completed
import uuid

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(f'test1_batch_load_recovery_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class BatchLoadRecoveryTest:
    def __init__(self):
        self.source_region = 'us-east-1'
        self.target_region = 'us-west-2'
        self.source_table = 'source-test-table'
        self.target_table = 'test1-recovery-table'
        
        # ä½¿ç”¨Lambdaå®é™…é…ç½®çš„S3æ¡¶åç§°
        import os
        self.backup_bucket = os.environ.get('TEST_S3_BUCKET')
        if not self.backup_bucket:
            raise ValueError("TEST_S3_BUCKETç¯å¢ƒå˜é‡æœªè®¾ç½®ï¼Œè¯·å…ˆè¿è¡Œrun_test1.sh")
        
        # DynamoDBå®¢æˆ·ç«¯
        self.ddb_source = boto3.client('dynamodb', region_name=self.source_region)
        self.ddb_target = boto3.client('dynamodb', region_name=self.target_region)
        self.s3 = boto3.client('s3', region_name=self.target_region)
        
        # æµ‹è¯•å‚æ•°
        self.total_records = 100000
        self.target_tps = 100
        self.batch_size = 25  # DynamoDB batch_write_itemé™åˆ¶
        
        # çŠ¶æ€è·Ÿè¸ª
        self.loaded_count = 0
        self.export_arn = None
        self.export_start_time = None
        self.load_complete_time = None
        
    def cleanup_environment(self):
        """æ¸…ç†æµ‹è¯•ç¯å¢ƒ"""
        logger.info("ğŸ§¹ æ¸…ç†æµ‹è¯•ç¯å¢ƒ...")
        
        # åˆ é™¤æºè¡¨
        try:
            self.ddb_source.delete_table(TableName=self.source_table)
            logger.info(f"âœ… åˆ é™¤æºè¡¨: {self.source_table}")
            
            # ç­‰å¾…è¡¨åˆ é™¤å®Œæˆ
            waiter = self.ddb_source.get_waiter('table_not_exists')
            waiter.wait(TableName=self.source_table)
        except Exception as e:
            if 'ResourceNotFoundException' not in str(e):
                logger.error(f"åˆ é™¤æºè¡¨å¤±è´¥: {e}")
        
        # åˆ é™¤ç›®æ ‡è¡¨
        try:
            self.ddb_target.delete_table(TableName=self.target_table)
            logger.info(f"âœ… åˆ é™¤ç›®æ ‡è¡¨: {self.target_table}")
            
            # ç­‰å¾…è¡¨åˆ é™¤å®Œæˆ
            waiter = self.ddb_target.get_waiter('table_not_exists')
            waiter.wait(TableName=self.target_table)
        except Exception as e:
            if 'ResourceNotFoundException' not in str(e):
                logger.error(f"åˆ é™¤ç›®æ ‡è¡¨å¤±è´¥: {e}")
        
        # æ¸…ç†S3æ¡¶
        self.cleanup_s3_buckets()
    
    def cleanup_s3_buckets(self):
        """æ¸…ç†ç›¸å…³S3æ¡¶"""
        try:
            # åˆ—å‡ºæ‰€æœ‰ä»¥test-cross-region-backupå¼€å¤´çš„æ¡¶
            response = self.s3.list_buckets()
            for bucket in response['Buckets']:
                bucket_name = bucket['Name']
                if bucket_name.startswith('test-cross-region-backup'):
                    logger.info(f"ğŸ—‘ï¸ æ¸…ç†S3æ¡¶: {bucket_name}")
                    
                    # åˆ é™¤æ¡¶ä¸­æ‰€æœ‰å¯¹è±¡
                    try:
                        objects = self.s3.list_objects_v2(Bucket=bucket_name)
                        if 'Contents' in objects:
                            delete_keys = [{'Key': obj['Key']} for obj in objects['Contents']]
                            self.s3.delete_objects(
                                Bucket=bucket_name,
                                Delete={'Objects': delete_keys}
                            )
                        
                        # åˆ é™¤æ¡¶
                        self.s3.delete_bucket(Bucket=bucket_name)
                        logger.info(f"âœ… å·²åˆ é™¤S3æ¡¶: {bucket_name}")
                    except Exception as e:
                        logger.error(f"åˆ é™¤S3æ¡¶å¤±è´¥ {bucket_name}: {e}")
        except Exception as e:
            logger.error(f"æ¸…ç†S3æ¡¶å¤±è´¥: {e}")

    def create_source_table(self):
        """åˆ›å»ºæºè¡¨"""
        try:
            self.ddb_source.create_table(
                TableName=self.source_table,
                KeySchema=[
                    {'AttributeName': 'id', 'KeyType': 'HASH'}
                ],
                AttributeDefinitions=[
                    {'AttributeName': 'id', 'AttributeType': 'S'}
                ],
                BillingMode='PAY_PER_REQUEST',
                StreamSpecification={
                    'StreamEnabled': True,
                    'StreamViewType': 'NEW_AND_OLD_IMAGES'
                }
            )
            
            # ç­‰å¾…è¡¨åˆ›å»ºå®Œæˆ
            waiter = self.ddb_source.get_waiter('table_exists')
            waiter.wait(TableName=self.source_table)
            
            # å¯ç”¨Point-in-Time Recovery
            self.ddb_source.update_continuous_backups(
                TableName=self.source_table,
                PointInTimeRecoverySpecification={
                    'PointInTimeRecoveryEnabled': True
                }
            )
            
            logger.info(f"âœ… åˆ›å»ºæºè¡¨: {self.source_table} (å·²å¯ç”¨PITR)")
            return True
        except Exception as e:
            if 'ResourceInUseException' in str(e):
                logger.info(f"æºè¡¨å·²å­˜åœ¨: {self.source_table}")
                return True
            logger.error(f"âŒ åˆ›å»ºæºè¡¨å¤±è´¥: {e}")
            return False
    
    def create_s3_bucket(self):
        """æ£€æŸ¥S3å­˜å‚¨æ¡¶æ˜¯å¦å­˜åœ¨"""
        try:
            self.s3.head_bucket(Bucket=self.backup_bucket)
            logger.info(f"âœ… ä½¿ç”¨ç°æœ‰S3æ¡¶: {self.backup_bucket}")
            return True
        except Exception as e:
            logger.error(f"âŒ S3æ¡¶ä¸å­˜åœ¨: {e}")
            return False
    
    def create_target_table(self):
        """åˆ›å»ºç›®æ ‡æ¢å¤è¡¨"""
        try:
            # è·å–æºè¡¨ç»“æ„
            source_desc = self.ddb_source.describe_table(TableName=self.source_table)
            table_def = source_desc['Table']
            
            # åˆ›å»ºç›®æ ‡è¡¨
            self.ddb_target.create_table(
                TableName=self.target_table,
                KeySchema=table_def['KeySchema'],
                AttributeDefinitions=table_def['AttributeDefinitions'],
                BillingMode='PAY_PER_REQUEST'
            )
            
            # ç­‰å¾…è¡¨åˆ›å»ºå®Œæˆ
            waiter = self.ddb_target.get_waiter('table_exists')
            waiter.wait(TableName=self.target_table)
            logger.info(f"âœ… åˆ›å»ºç›®æ ‡è¡¨: {self.target_table}")
            return True
        except Exception as e:
            if 'ResourceInUseException' in str(e):
                logger.info(f"ç›®æ ‡è¡¨å·²å­˜åœ¨: {self.target_table}")
                return True
            logger.error(f"âŒ åˆ›å»ºç›®æ ‡è¡¨å¤±è´¥: {e}")
            return False
    
    def generate_batch_data(self, start_id, count):
        """ç”Ÿæˆæ‰¹é‡æ•°æ®"""
        items = []
        for i in range(count):
            item_id = start_id + i
            items.append({
                'PutRequest': {
                    'Item': {
                        'id': {'S': f'batch-{item_id:06d}'},
                        'data': {'S': f'Test data for item {item_id}'},
                        'timestamp': {'S': datetime.now(timezone.utc).isoformat()},
                        'batch_id': {'N': str(item_id // 1000)},
                        'test_type': {'S': 'batch_load_test'}
                    }
                }
            })
        return items
    
    def write_batch(self, items):
        """å†™å…¥ä¸€æ‰¹æ•°æ®"""
        try:
            response = self.ddb_source.batch_write_item(
                RequestItems={
                    self.source_table: items
                }
            )
            
            # å¤„ç†æœªå¤„ç†çš„é¡¹ç›®
            unprocessed = response.get('UnprocessedItems', {})
            retry_count = 0
            while unprocessed and retry_count < 3:
                time.sleep(0.1 * (2 ** retry_count))  # æŒ‡æ•°é€€é¿
                response = self.ddb_source.batch_write_item(RequestItems=unprocessed)
                unprocessed = response.get('UnprocessedItems', {})
                retry_count += 1
            
            return len(items)
        except Exception as e:
            logger.error(f"æ‰¹é‡å†™å…¥å¤±è´¥: {e}")
            return 0
    
    def batch_load_worker(self, thread_id, batches_per_thread):
        """æ‰¹é‡åŠ è½½å·¥ä½œçº¿ç¨‹"""
        loaded = 0
        for batch_num in range(batches_per_thread):
            start_id = thread_id * batches_per_thread * self.batch_size + batch_num * self.batch_size
            items = self.generate_batch_data(start_id, self.batch_size)
            
            written = self.write_batch(items)
            loaded += written
            self.loaded_count += written
            
            # æ§åˆ¶TPS
            time.sleep(self.batch_size / self.target_tps)
            
            if self.loaded_count % 5000 == 0:
                logger.info(f"å·²åŠ è½½: {self.loaded_count:,} æ¡è®°å½•")
        
        return loaded
    
    def start_batch_loading(self):
        """å¼€å§‹æ‰¹é‡æ•°æ®åŠ è½½"""
        logger.info(f"ğŸš€ å¼€å§‹æ‰¹é‡åŠ è½½ {self.total_records:,} æ¡è®°å½• (TPS: {self.target_tps})")
        
        # è®¡ç®—çº¿ç¨‹å’Œæ‰¹æ¬¡
        num_threads = 4
        total_batches = self.total_records // self.batch_size
        batches_per_thread = total_batches // num_threads
        
        start_time = time.time()
        
        with ThreadPoolExecutor(max_workers=num_threads) as executor:
            futures = []
            for thread_id in range(num_threads):
                future = executor.submit(self.batch_load_worker, thread_id, batches_per_thread)
                futures.append(future)
            
            # ç­‰å¾…æ‰€æœ‰çº¿ç¨‹å®Œæˆ
            for future in as_completed(futures):
                try:
                    result = future.result()
                    logger.info(f"çº¿ç¨‹å®Œæˆï¼ŒåŠ è½½äº† {result} æ¡è®°å½•")
                except Exception as e:
                    logger.error(f"çº¿ç¨‹æ‰§è¡Œå¤±è´¥: {e}")
        
        self.load_complete_time = datetime.now(timezone.utc)
        elapsed = time.time() - start_time
        actual_tps = self.loaded_count / elapsed
        
        logger.info(f"âœ… æ‰¹é‡åŠ è½½å®Œæˆ:")
        logger.info(f"   æ€»è®°å½•æ•°: {self.loaded_count:,}")
        logger.info(f"   è€—æ—¶: {elapsed:.1f} ç§’")
        logger.info(f"   å®é™…TPS: {actual_tps:.1f}")
    
    def trigger_export_during_load(self):
        """åœ¨åŠ è½½è¿‡ç¨‹ä¸­è§¦å‘å…¨é‡å¤‡ä»½"""
        # ç­‰å¾…åŠ è½½åˆ°50%æ—¶è§¦å‘Export
        while self.loaded_count < self.total_records * 0.5:
            time.sleep(5)
        
        logger.info(f"ğŸ”„ è§¦å‘å…¨é‡å¤‡ä»½ (å·²åŠ è½½: {self.loaded_count:,} æ¡)")
        self.export_start_time = datetime.now(timezone.utc)
        
        try:
            # åŠ¨æ€è·å–å½“å‰è´¦æˆ·ID
            sts = boto3.client('sts')
            account_id = sts.get_caller_identity()['Account']
            
            response = self.ddb_source.export_table_to_point_in_time(
                TableArn=f'arn:aws:dynamodb:{self.source_region}:{account_id}:table/{self.source_table}',
                S3Bucket=self.backup_bucket,
                S3Prefix=f'full-backups/{self.source_table}/{self.export_start_time.strftime("%Y%m%d_%H%M%S")}/',
                ExportFormat='DYNAMODB_JSON'
            )
            
            self.export_arn = response['ExportDescription']['ExportArn']
            logger.info(f"âœ… Exportå·²å¯åŠ¨: {self.export_arn}")
            
            # ä¿å­˜å¤‡ä»½å…ƒæ•°æ®
            s3_path = f's3://{self.backup_bucket}/full-backups/{self.source_table}/{self.export_start_time.strftime("%Y%m%d_%H%M%S")}/'
            metadata = {
                'export_arn': self.export_arn,
                'export_time': self.export_start_time.isoformat(),
                'table_name': self.source_table,
                'records_loaded_at_export': self.loaded_count,
                's3_path': s3_path
            }
            
            metadata_key = f'backup-metadata/{self.source_table}/test1_export_{self.export_start_time.strftime("%Y%m%d_%H%M%S")}.json'
            self.s3.put_object(
                Bucket=self.backup_bucket,
                Key=metadata_key,
                Body=json.dumps(metadata, indent=2)
            )
            
        except Exception as e:
            logger.error(f"âŒ Exportå¯åŠ¨å¤±è´¥: {e}")
    
    def wait_for_export_completion(self):
        """ç­‰å¾…Exportå®Œæˆ"""
        if not self.export_arn:
            return False
        
        logger.info("â³ ç­‰å¾…Exportå®Œæˆ...")
        while True:
            try:
                response = self.ddb_source.describe_export(ExportArn=self.export_arn)
                status = response['ExportDescription']['ExportStatus']
                
                if status == 'COMPLETED':
                    logger.info("âœ… Exportå®Œæˆ")
                    return True
                elif status == 'FAILED':
                    logger.error("âŒ Exportå¤±è´¥")
                    return False
                else:
                    logger.info(f"ExportçŠ¶æ€: {status}")
                    time.sleep(30)
            except Exception as e:
                logger.error(f"æ£€æŸ¥ExportçŠ¶æ€å¤±è´¥: {e}")
                time.sleep(30)
    
    def delete_target_table(self):
        """åˆ é™¤ç›®æ ‡è¡¨"""
        try:
            self.ddb_target.delete_table(TableName=self.target_table)
            logger.info(f"ğŸ—‘ï¸ åˆ é™¤ç›®æ ‡è¡¨: {self.target_table}")
            
            # ç­‰å¾…è¡¨åˆ é™¤å®Œæˆ
            waiter = self.ddb_target.get_waiter('table_not_exists')
            waiter.wait(TableName=self.target_table)
            logger.info("âœ… ç›®æ ‡è¡¨åˆ é™¤å®Œæˆ")
            return True
        except Exception as e:
            if 'ResourceNotFoundException' not in str(e):
                logger.error(f"åˆ é™¤ç›®æ ‡è¡¨å¤±è´¥: {e}")
                return False
            logger.info("ç›®æ ‡è¡¨ä¸å­˜åœ¨ï¼Œè·³è¿‡åˆ é™¤")
            return True

    def disaster_recovery(self):
        """ä½¿ç”¨disaster_recovery_managerè¿›è¡Œå®Œæ•´æ¢å¤ (å…¨é‡+å¢é‡)"""
        logger.info("ğŸš¨ å¼€å§‹ç¾éš¾æ¢å¤ (å…¨é‡+å¢é‡)...")
        
        try:
            from disaster_recovery_manager import DisasterRecoveryManager
            
            dr_manager = DisasterRecoveryManager(
                self.source_region,
                self.target_region, 
                self.backup_bucket
            )
            
            # æ‰§è¡Œå®Œæ•´çš„ç¾éš¾æ¢å¤ (å…¨é‡ + å¢é‡)
            success = dr_manager.full_disaster_recovery(
                self.source_table,
                self.target_table
            )
            
            if success:
                logger.info("âœ… ç¾éš¾æ¢å¤å®Œæˆ (å…¨é‡+å¢é‡)")
                return True
            else:
                logger.error("âŒ ç¾éš¾æ¢å¤å¤±è´¥")
                return False
                
        except Exception as e:
            logger.error(f"âŒ ç¾éš¾æ¢å¤æ‰§è¡Œå¤±è´¥: {e}")
            return False
    
    def verify_data_consistency(self):
        """éªŒè¯æ•°æ®ä¸€è‡´æ€§"""
        logger.info("ğŸ” éªŒè¯æ•°æ®ä¸€è‡´æ€§...")
        
        try:
            # è·å–æºè¡¨è®°å½•æ•°
            source_response = self.ddb_source.describe_table(TableName=self.source_table)
            source_count = source_response['Table']['ItemCount']
            
            # è·å–ç›®æ ‡è¡¨è®°å½•æ•°
            target_response = self.ddb_target.describe_table(TableName=self.target_table)
            target_count = target_response['Table']['ItemCount']
            
            logger.info(f"æºè¡¨è®°å½•æ•°: {source_count:,}")
            logger.info(f"ç›®æ ‡è¡¨è®°å½•æ•°: {target_count:,}")
            
            # ç²¾ç¡®è®¡æ•°éªŒè¯
            source_scan = self.ddb_source.scan(
                TableName=self.source_table,
                Select='COUNT'
            )
            actual_source_count = source_scan['Count']
            
            target_scan = self.ddb_target.scan(
                TableName=self.target_table,
                Select='COUNT'
            )
            actual_target_count = target_scan['Count']
            
            logger.info(f"æºè¡¨å®é™…è®°å½•æ•°: {actual_source_count:,}")
            logger.info(f"ç›®æ ‡è¡¨å®é™…è®°å½•æ•°: {actual_target_count:,}")
            
            # éªŒè¯ç»“æœ
            if actual_source_count == actual_target_count:
                logger.info("âœ… æ•°æ®ä¸€è‡´æ€§éªŒè¯é€šè¿‡")
                return True
            else:
                logger.error(f"âŒ æ•°æ®ä¸ä¸€è‡´: å·®å¼‚ {abs(actual_source_count - actual_target_count)} æ¡")
                return False
                
        except Exception as e:
            logger.error(f"âŒ éªŒè¯å¤±è´¥: {e}")
            return False
    
    def run_test(self):
        """è¿è¡Œå®Œæ•´æµ‹è¯•"""
        logger.info("=" * 60)
        logger.info("ğŸ§ª æµ‹è¯•1: æ‰¹é‡æ•°æ®åŠ è½½æ¢å¤æµ‹è¯• (ä½¿ç”¨disaster_recovery_manager)")
        logger.info("æµç¨‹: åˆ é™¤ç›®æ ‡è¡¨ -> disaster_recovery_managerå®Œæ•´æ¢å¤ (å…¨é‡+å¢é‡)")
        logger.info("=" * 60)
        
        test_results = {
            'test_name': 'batch_load_recovery',
            'start_time': datetime.now(timezone.utc).isoformat(),
            'parameters': {
                'total_records': self.total_records,
                'target_tps': self.target_tps,
                'source_table': self.source_table,
                'target_table': self.target_table,
                'backup_bucket': self.backup_bucket
            }
        }
        
        try:
            # 1. æ£€æŸ¥S3æ¡¶
            if not self.create_s3_bucket():
                raise Exception("S3æ¡¶æ£€æŸ¥å¤±è´¥")
            
            # 2. æ£€æŸ¥æºè¡¨æ˜¯å¦å­˜åœ¨
            logger.info("âœ… ä½¿ç”¨ç°æœ‰æºè¡¨: source-test-table")
            
            # 3. ç›®æ ‡è¡¨å°†åœ¨æ¢å¤è¿‡ç¨‹ä¸­è‡ªåŠ¨åˆ›å»º
            
            # 4. å¯åŠ¨Exportçº¿ç¨‹
            export_thread = threading.Thread(target=self.trigger_export_during_load)
            export_thread.start()
            
            # 5. å¼€å§‹æ‰¹é‡åŠ è½½
            self.start_batch_loading()
            
            # 7. ç­‰å¾…Exportçº¿ç¨‹å®Œæˆ
            export_thread.join()
            
            # 8. ç­‰å¾…Exportå®Œæˆ
            if not self.wait_for_export_completion():
                raise Exception("Exportæœªå®Œæˆ")
            
            # 9. ä»exportæ¢å¤ï¼ˆdisaster_recovery_managerä¼šè‡ªåŠ¨å¤„ç†ç›®æ ‡è¡¨å’Œå¢é‡ï¼‰
            if not self.disaster_recovery():
                raise Exception("ç¾éš¾æ¢å¤å¤±è´¥")
            
            # 10. éªŒè¯æ•°æ®ä¸€è‡´æ€§
            consistency_ok = self.verify_data_consistency()
            
            # è®°å½•ç»“æœ
            test_results.update({
                'end_time': datetime.now(timezone.utc).isoformat(),
                'loaded_records': self.loaded_count,
                'export_arn': self.export_arn,
                'export_start_time': self.export_start_time.isoformat() if self.export_start_time else None,
                'load_complete_time': self.load_complete_time.isoformat() if self.load_complete_time else None,
                'consistency_verified': consistency_ok,
                'status': 'SUCCESS' if consistency_ok else 'FAILED'
            })
            
            # ä¿å­˜æµ‹è¯•ç»“æœ
            result_file = f'test1_results_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json'
            with open(result_file, 'w') as f:
                json.dump(test_results, f, indent=2)
            
            logger.info("=" * 60)
            if consistency_ok:
                logger.info("ğŸ‰ æµ‹è¯•1å®Œæˆ - æˆåŠŸ!")
            else:
                logger.info("âŒ æµ‹è¯•1å®Œæˆ - å¤±è´¥!")
            logger.info(f"è¯¦ç»†ç»“æœ: {result_file}")
            logger.info("=" * 60)
            
        except Exception as e:
            logger.error(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
            test_results.update({
                'end_time': datetime.now(timezone.utc).isoformat(),
                'error': str(e),
                'status': 'ERROR'
            })

if __name__ == '__main__':
    test = BatchLoadRecoveryTest()
    test.run_test()

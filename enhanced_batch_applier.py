#!/usr/bin/env python3
"""
Enhanced Batch Applier with Retry Mechanism
"""

import boto3
import json
import argparse
import logging
import time
from datetime import datetime
from botocore.exceptions import ClientError
from boto3.dynamodb.types import TypeDeserializer
from retry_decorator import retry_dynamodb_operation, retry_s3_operation

class EnhancedBatchApplier:
    def __init__(self, target_table_name: str, region: str = 'us-west-2', log_suffix: str = None):
        self.target_table_name = target_table_name
        self.region = region
        self.deserializer = TypeDeserializer()
        
        # ä½¿ç”¨ç»Ÿä¸€çš„æ—¥å¿—æ–‡ä»¶
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        self.error_log_file = f"apply_changes_{target_table_name}_{timestamp}.log"
        
        # é…ç½®æ—¥å¿— - ä½¿ç”¨appendæ¨¡å¼é¿å…è¦†ç›–
        if not logging.getLogger().handlers:
            logging.basicConfig(
                level=logging.INFO,
                format='%(asctime)s - %(levelname)s - %(message)s',
                handlers=[
                    logging.FileHandler(self.error_log_file, mode='a'),
                    logging.StreamHandler()
                ]
            )
        self.logger = logging.getLogger(__name__)
        
        # åˆå§‹åŒ–AWSå®¢æˆ·ç«¯
        self.s3 = boto3.client('s3', region_name=region)
        self.dynamodb = boto3.resource('dynamodb', region_name=region)
        self.table = self.dynamodb.Table(target_table_name)
        
        # éªŒè¯è¡¨å­˜åœ¨
        self._verify_table()
    
    @retry_dynamodb_operation
    def _verify_table(self):
        """éªŒè¯ç›®æ ‡è¡¨å­˜åœ¨"""
        try:
            self.table.load()
            self.logger.info(f"âœ… ç›®æ ‡è¡¨ {self.target_table_name} éªŒè¯æˆåŠŸ")
        except ClientError:
            raise ValueError(f"âŒ è¡¨ {self.target_table_name} ä¸å­˜åœ¨")
    
    @retry_s3_operation
    def _read_s3_file(self, bucket: str, key: str) -> list:
        """ä»S3è¯»å–æ–‡ä»¶ï¼Œå¸¦é‡è¯•"""
        obj = self.s3.get_object(Bucket=bucket, Key=key)
        records = json.loads(obj['Body'].read())
        self.logger.info(f"ğŸ“„ ä»S3è¯»å– {len(records)} æ¡è®°å½•")
        return records
    
    @retry_dynamodb_operation
    def _apply_batch_with_retry(self, batch_records: list) -> dict:
        """åº”ç”¨å•ä¸ªæ‰¹æ¬¡ï¼Œå¸¦é‡è¯•æœºåˆ¶"""
        stats = {'applied': 0, 'errors': 0}
        error_records = []
        
        # ä½¿ç”¨ç›´æ¥å†™å…¥ï¼Œä¸ä½¿ç”¨batch_writerçš„å»é‡é€»è¾‘
        dynamodb_client = boto3.client('dynamodb', region_name=self.table.meta.client.meta.region_name)
        
        for i, record in enumerate(batch_records):
            try:
                event = record['eventName']
                
                if event in ['INSERT', 'MODIFY']:
                    # INSERT/MODIFYç»Ÿä¸€ä½¿ç”¨å¹‚ç­‰put_item
                    # å‰æï¼šNewImageåŒ…å«å®Œæ•´è®°å½•
                    item = {k: self.deserializer.deserialize(v) 
                           for k, v in record['dynamodb']['NewImage'].items()}
                    
                    from boto3.dynamodb.types import TypeSerializer
                    serializer = TypeSerializer()
                    ddb_item = {k: serializer.serialize(v) for k, v in item.items()}
                    
                    # å¹‚ç­‰PUTæ“ä½œ
                    dynamodb_client.put_item(
                        TableName=self.target_table_name,
                        Item=ddb_item
                    )
                    stats['applied'] += 1
                    
                elif event == 'REMOVE':
                    # REMOVEæ“ä½œ - åˆ é™¤ç°æœ‰è®°å½•
                    key_attrs = {k: self.deserializer.deserialize(v) 
                               for k, v in record['dynamodb']['Keys'].items()}
                    
                    from boto3.dynamodb.types import TypeSerializer
                    serializer = TypeSerializer()
                    ddb_key = {k: serializer.serialize(v) for k, v in key_attrs.items()}
                    
                    try:
                        dynamodb_client.delete_item(
                            TableName=self.target_table_name,
                            Key=ddb_key,
                            ConditionExpression='attribute_exists(#pk)',
                            ExpressionAttributeNames={'#pk': list(key_attrs.keys())[0]}
                        )
                        stats['applied'] += 1
                    except ClientError as e:
                        if e.response['Error']['Code'] == 'ConditionalCheckFailedException':
                            self.logger.info(f"REMOVEè·³è¿‡ - è®°å½•ä¸å­˜åœ¨")
                            stats['applied'] += 1
                        else:
                            raise
                    
            except Exception as e:
                stats['errors'] += 1
                error_msg = f"è®°å½• {i}: {str(e)}"
                self.logger.error(error_msg)
                
                error_records.append({
                    'record_index': i,
                    'error': str(e),
                    'record_data': record
                })
        
        # ä¿å­˜é”™è¯¯è®°å½•
        if error_records:
            error_file = f"batch_errors_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            with open(error_file, 'w') as f:
                json.dump(error_records, f, indent=2, default=str)
            self.logger.warning(f"âš ï¸ æ‰¹æ¬¡é”™è¯¯è¯¦æƒ…ä¿å­˜åˆ°: {error_file}")
        
        return stats
    
    def apply_changes_from_s3(self, s3_file_path: str, batch_size: int = 100) -> bool:
        """ä»S3æ–‡ä»¶åº”ç”¨å˜æ›´ï¼Œå¸¦å®Œæ•´é‡è¯•æœºåˆ¶"""
        try:
            # è§£æS3è·¯å¾„
            bucket, key = s3_file_path.replace('s3://', '').split('/', 1)
            
            # è¯»å–S3æ–‡ä»¶ (å¸¦é‡è¯•)
            records = self._read_s3_file(bucket, key)
            
            if not records:
                self.logger.info("ğŸ“„ æ–‡ä»¶ä¸ºç©ºï¼Œæ— éœ€å¤„ç†")
                return True
            
            # åˆ†æ‰¹å¤„ç†è®°å½•
            total_stats = {'applied': 0, 'errors': 0}
            total_batches = (len(records) + batch_size - 1) // batch_size
            
            for i in range(0, len(records), batch_size):
                batch_records = records[i:i + batch_size]
                batch_num = i // batch_size + 1
                
                self.logger.info(f"ğŸ”„ å¤„ç†æ‰¹æ¬¡ {batch_num}/{total_batches} ({len(batch_records)} æ¡è®°å½•)")
                
                # åº”ç”¨æ‰¹æ¬¡ (å¸¦é‡è¯•)
                max_batch_retries = 3
                batch_success = False
                
                for retry_attempt in range(max_batch_retries):
                    try:
                        batch_stats = self._apply_batch_with_retry(batch_records)
                        
                        # ç´¯è®¡ç»Ÿè®¡
                        total_stats['applied'] += batch_stats['applied']
                        total_stats['errors'] += batch_stats['errors']
                        
                        batch_success = True
                        break
                        
                    except Exception as e:
                        if retry_attempt < max_batch_retries - 1:
                            delay = 2 ** retry_attempt  # æŒ‡æ•°é€€é¿
                            self.logger.warning(f"âš ï¸ æ‰¹æ¬¡ {batch_num} å¤±è´¥ï¼Œ{delay}ç§’åé‡è¯•: {e}")
                            time.sleep(delay)
                        else:
                            self.logger.error(f"âŒ æ‰¹æ¬¡ {batch_num} æœ€ç»ˆå¤±è´¥: {e}")
                            total_stats['errors'] += len(batch_records)
                
                if not batch_success:
                    self.logger.error(f"âŒ æ‰¹æ¬¡ {batch_num} å¤„ç†å¤±è´¥")
            
            # è¾“å‡ºæœ€ç»ˆç»Ÿè®¡
            success_rate = (total_stats['applied'] / len(records)) * 100 if records else 100
            self.logger.info(f"ğŸ“Š å¤„ç†å®Œæˆ: {total_stats['applied']} æˆåŠŸ, {total_stats['errors']} å¤±è´¥ ({success_rate:.1f}%)")
            
            if total_stats['errors'] > 0:
                self.logger.warning(f"âš ï¸ é”™è¯¯æ—¥å¿—: {self.error_log_file}")
            
            return total_stats['errors'] == 0
            
        except Exception as e:
            self.logger.error(f"âŒ åº”ç”¨å˜æ›´å¤±è´¥: {e}")
            return False

def main():
    parser = argparse.ArgumentParser(description='å¢å¼ºç‰ˆDynamoDBå˜æ›´åº”ç”¨ (å¸¦é‡è¯•)')
    parser.add_argument('--s3-file-path', required=True, help='S3æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--target-table', required=True, help='ç›®æ ‡è¡¨å')
    parser.add_argument('--region', default='us-west-2', help='AWSåŒºåŸŸ')
    parser.add_argument('--batch-size', type=int, default=100, help='æ‰¹å¤„ç†å¤§å°')
    
    args = parser.parse_args()
    
    applier = EnhancedBatchApplier(args.target_table, args.region)
    success = applier.apply_changes_from_s3(args.s3_file_path, args.batch_size)
    
    if success:
        print("âœ… å˜æ›´åº”ç”¨æˆåŠŸ")
        return 0
    else:
        print("âŒ å˜æ›´åº”ç”¨å¤±è´¥")
        return 1

if __name__ == "__main__":
    exit(main())
